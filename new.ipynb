{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "23a5e9c4",
      "metadata": {},
      "source": [
        "## All imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "acc4558b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/reasoning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a28ba41d",
      "metadata": {},
      "source": [
        "## Load tokenizer and model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ae2ee16d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "Loading weights: 100%|██████████| 311/311 [00:00<00:00, 608.50it/s, Materializing param=model.norm.weight]                              \n",
            "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
          ]
        }
      ],
      "source": [
        "model_id = \"Qwen/Qwen3-0.6B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb9ad73",
      "metadata": {},
      "source": [
        "## Basic Inspection of model we have loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f4d89609",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qwen3ForCausalLM(\n",
            "  (model): Qwen3Model(\n",
            "    (embed_tokens): Embedding(151936, 1024)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "    (rotary_emb): Qwen3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30e3840d",
      "metadata": {},
      "source": [
        "## write a basic generation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "01272ebf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " a. washington b. washington dc c. washington dc d. washington\n",
            "Answer:\n",
            "The capital of the United States is **Washington, D.C.**. The answer is **B**.\n",
            "\n",
            "**Explanation:**\n",
            "The United States is a country located in the United States, and its capital is Washington, D.C. The capital is also known as the capital city of the country. The capital city is also known as the capital of the United States. The answer is **B**.\n",
            "\n",
            "**Answer\n"
          ]
        }
      ],
      "source": [
        "prompt = \"what is capital of united states?\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "def generate_basic_loop(prompts, max_tokens = 100):\n",
        "    '''\n",
        "    Step1: convert the input list of tokens into tensors\n",
        "    Step2: convert it to tensors and unsqueeze at position 0\n",
        "    '''\n",
        "\n",
        "    input_list_ids = tokenizer.encode(prompt)\n",
        "    input_tensor_ids = torch.tensor(input_list_ids, device = device).unsqueeze(0)\n",
        "    response_list_ids = []\n",
        "    for i in range(max_tokens):\n",
        "        #print(input_tensor_ids.shape)\n",
        "        logits = model(input_tensor_ids).logits\n",
        "        #print(f'logits are {logits}')\n",
        "        #print(f'logits shape is {logits.shape}')\n",
        "        last_token = logits[:, -1, :].argmax(dim = -1, keepdim = True).detach()\n",
        "        #print(f'last token is {last_token}')\n",
        "        response_list_ids.extend(last_token.tolist()[0])\n",
        "\n",
        "        input_tensor_ids = torch.cat([input_tensor_ids, last_token], dim = 1)\n",
        "    return response_list_ids\n",
        "print(tokenizer.decode(generate_basic_loop(prompt)))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b86dcd77",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[151643]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode('<|endoftext|>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8498a945",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "?\n",
            "The answer is India's capital. Let me know if I can help.\n",
            "The answer is India's capital.\n",
            "The answer is India's capital.\n",
            "The answer is India's capital.\n",
            "The answer is India's capital.\n",
            "The answer is India's capital\n"
          ]
        }
      ],
      "source": [
        "prompt = \"what is the capital of india?\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "out = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff083d5e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4340, 653, 600, 3164, 264, 1809, 30]\n",
            "torch.Size([7])\n"
          ]
        }
      ],
      "source": [
        "prompt = \"How do i win a game?\"\n",
        "input_list_ids = tokenizer.encode(prompt)\n",
        "print(input_list_ids)\n",
        "input_tensor_ids = torch.tensor(input_list_ids)\n",
        "print(input_tensor_ids.shape)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
